{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oHCh06-7Yuvo","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"error","timestamp":1757555076821,"user_tz":240,"elapsed":32,"user":{"displayName":"Alvaro Fabián Baldiviezo Rodríguez","userId":"13769536189324427176"}},"outputId":"e1823c10-b6e3-45e1-8a85-47204c8c67e7"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (ipython-input-424201401.py, line 96)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-424201401.py\"\u001b[0;36m, line \u001b[0;32m96\u001b[0m\n\u001b[0;31m    def build_model(input dim: int) -> tf.keras.Model:\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["# Import y Configuracion\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n","\n","from tensorflow.keras import layers, models, callbacks\n","from sklearn.pipeline import Pipeline\n","\n","# Reproducibilidad: Sirve para evitar datos duplicados. De esta manera, no sale otro dato si vuelvo a correr el codigo.\n","# Inicializacion aleatoria de pesos\n","# Seleccion de lotes (batches) por epach\n","# Division de Train - Test\n","# Calculo de GPU (no determinista)\n","SEED = 42\n","os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)\n","\n","print(\"TensorFlow: \", tf.__version__)\n","\n","# 1. Cargar el Dataset Titanic\n","\n","# Fuente Abierta\n","URL = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n","df = pd.read_csv(URL)\n","\n","print(\"Shape bruto: \", df.shape)\n","print(\"Cols: \", list(df.columns))\n","\n","# 2. Seleccion de variables y objetivo (y)\n","# Objetivo: Survived (0/1)\n","# Cuando la columna es muy obvia, o no dice nada o nada para predecir, puede haber una fuga o error. Si tenemos varias columnas, no usamos todas necesariamente.\n","y = df[\"Survived\"].astype(int).values\n","\n","#Tomamos columnas usuales y evitamos leakage (PassengerId, name, Ticket, CABIN NO ayudan de base)\n","\n","X = df[[\n","    \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"\n","]].copy()\n","\n","# 3. Feature engineering liviano y limpio\n","\n","# 3.1 Imputacion simple (dejar NaN y luego imputamos con SimpleImputer)\n","# 3.2 Crear FamilySize y IsAlone (suma de SibSp+Parch y binario)\n","\n","X[\"FamilySize\"] = X[\"SibSp\"].fillna(0) + X[\"Parch\"].fillna(0) + 1\n","X[\"IsAlone\"] = (X[\"FamilySize\"] == 1).astype(int)\n","\n","# 4. Definicion de columnas por tipo\n","num_cols = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"FamilySize\"]\n","cat_cols = [\"Pclass\", \"Sex\", \"Embarked\"] # Pclass como categorica ayuda a One-Hot\n","\n","# 5. Preprocesamiento con ColumnTransformer\n","# Para el tratamiento, solemos utilizar: Media, mediana y/o moda\n","numeric_transformer = Pipeline(steps=[\n","    (\"imputer\", SimpleImputer(strategy=\"median\")),\n","    (\"scaler\", StandardScaler()) # denso; aqui si con media\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n","    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n","])\n","\n","from sklearn.pipeline import Pipeline\n","preprocess = ColumnTransformer(\n","    transformers=[\n","        (\"num\", numeric_transformer, num_cols),\n","        (\"cat\", categorical_transformer, cat_cols)\n","    ],\n","    remainder=\"drop\"\n","\n",")\n","# 6. Split Train/Test estratificado\n","X_train_df, X_test_df, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=SEED, stratify=y\n",")\n","\n","#Ajustar transformadores en train y transformar ambos\n","X_train = preprocess.fit_transform(X_train_df)\n","X_test = preprocess.transform(X_test_df)\n","\n","X_train = X_train.astype(\"float32\")\n","X_test = X_test.astype(\"float32\")\n","\n","print(\"Input dims: \", X_train.shape[1])\n","\n","# 7. Definir y Compilar el modelo\n","def build_model(input dim: int) -> tf.keras.Model:\n","  model = models.Sequential([\n","      layers.Input(shape=(input_dim,)),\n","      layers.Dense(32, activation=\"relu\"),\n","      layers.Dropout(0.15), #Apaga o desactiva ese porcentaje de neuronas. Es para evitar sobre ajustes. La IA debe aprender, no memorizar.\n","      layers.Dense(16, activation=\"relu\"),\n","      layers.Dropout(0.15),\n","      layers.Dense(1, activation=\"sigmoid\")\n","  ])\n","  model.compile(\n","      optimizer=\"adam\",\n","      loss=\"binary_crossentropy\", #perdida binaria\n","      metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")] #accuracy del modelo\n","  )\n","  return model\n","\n","model = build_model(X_train.shape[1])\n","model.summary()\n","\n","# 8. Callbacks (buenas practicas)\n","# Guarda los buenos resultados. Es para que pueda quedarse con el mejor dato para el modelo. No se quiere el ultimo, si no el mejor. Es por eso que se realiza callback, para guardar resultados\n","\n","cbs = [\n","    callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=12, restore_best_weights=True),\n","    callbacks.ModelCheckpoint(filepath=\"titanic_best.keras\", monitor=\"val_auc\", mode=\"max\", save_best_only=True), #Se realiza un checkpoint o guardado para el callback\n","    callbacks.ReduceLROnPlateau(monitor=\"val_auc\", factor=0.5, patience=6)\n","]\n","\n","# 9. Entrenamiento\n","hist = model.fit(\n","    X_train, y_train,\n","    validation_split=0.2,\n","    batch_size=32,\n","    epochs=200,\n","    callbacks=cbs,\n","    verbose=1\n",")\n","\n","# 10. Evaluacion en test\n","y_proba = model.predict(X_test).ravel()\n","y_pred = (y_proba >= 0.5).astype(int)\n","\n","print(\"\\nMatriz de confusion:\\n\", confusion_matrix(y_test, y_pred))\n","print(\"\\nReporte de clasificacion:\\n\", classification_report(y_test, y_pred, digits=4))\n","print(\"\\nAUC: \", roc_auc_score(y_test, y_proba))\n","\n","# 11. Indiferencia robusta: funcion predict_one(dict de entrada)\n","def predict_one(sample: dict) -> float:\n","  \"\"\"\n","  Recibe un diccionario 'crudo' con las llaves esperadas:\n","  Pclass, Sex, Age, SibSp, Parch, Fare, Embarked\n","  (FamilySize e IsAlone se calculan internamente).\n","  Devuelve probabilidad de supervivencia.\n","  \"\"\"\n","\n","  #Convertir a DataFrame con columnas en orden esperado\n","  s = pd.DataFrame([sample])\n","  # Feature engineering consistente\n","  s[\"FamilySize\"] = s[\"SibSp\"].fillna(0) + s[\"Parch\"].fillna(0) + 1\n","  s[\"IsAlone\"] = (s[\"FamilySize\"] == 1).astype(int)\n","  # Aplicar EXACTAMENTE el mismo preprocesamiento\n","  s_proc = preprocess.transform(s[X.columns]) #mismas columnas base\n","  s_proc = s_proc.astype(\"float32\")\n","  #Predecir\n","  proba = model.predict(s_proc).item()\n","  return proba\n","\n","  #Ejemplo de indiferencia\n","  sample = {\n","      \"Pclass\": 3, #1, 2 o3\n","      \"Sex\": \"male\",#male / female\n","      \"Age\": 25,\n","      \"SibSp\": 1,\n","      \"Parch\": 0,\n","      \"Fare\": 7.25,\n","      \"Embarked\": \"S\" #S, C o Q\n","  }\n","  proba = predict_one(sample)\n","  print(f\"Probabilidad de supervivencia: {proba:.4f}\")\n","  print(\"Sobrevive\" if proba >= 0.5 else \"No sobrevive\")\n","\n","  #"]},{"cell_type":"code","source":["# Mas capas para más precisión o para qaue esté equilibrado. Generalmente es ensayo y error\n"],"metadata":{"id":"QqeqQYvNci1n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"6xi1y0rDnrjL"}}]}