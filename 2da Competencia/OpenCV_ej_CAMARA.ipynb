{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oow6Hd14kD6G"
      },
      "outputs": [],
      "source": [
        "# Notebook 6: Detección y reconocimiento de caras EN VIDEOS\n",
        "\n",
        "# Importamos las librerias necesarias\n",
        "import cv2 # Importamos OpenCV para procesamiento de imágenes\n",
        "import numpy as np # Importamos numpy para manejo de arrays\n",
        "import matplotlib.pyplot as plt # Importamos matplotlib para visualización\n",
        "from google.colab.patches import cv2_imshow # Importamos cv2_imshow para mostrar imágenes en Colab\n",
        "# Importar la camara de mi computadora\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import google"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Descargar los clasificadores en cascada pre entrenados para detección facial\n",
        "# Estos son archivos XML con patrones pre entrenados para detección\n",
        "!wget -O haarcascade_frontalface_default.xml https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
        "!wget -O haarcascade_eye.xml https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_eye.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fclz1EtxkMw5",
        "outputId": "2da38443-204f-492c-909f-975a54f702fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-06 22:27:49--  https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 930127 (908K) [text/plain]\n",
            "Saving to: ‘haarcascade_frontalface_default.xml’\n",
            "\n",
            "\r          haarcasca   0%[                    ]       0  --.-KB/s               \rhaarcascade_frontal 100%[===================>] 908.33K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-10-06 22:27:49 (18.9 MB/s) - ‘haarcascade_frontalface_default.xml’ saved [930127/930127]\n",
            "\n",
            "--2025-10-06 22:27:49--  https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_eye.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 341406 (333K) [text/plain]\n",
            "Saving to: ‘haarcascade_eye.xml’\n",
            "\n",
            "haarcascade_eye.xml 100%[===================>] 333.40K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-10-06 22:27:49 (9.67 MB/s) - ‘haarcascade_eye.xml’ saved [341406/341406]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los clasificadores cascada\n",
        "# Cargamos el clasificador para rostros frontales\n",
        "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "# Cargamso el clasificador para ojos\n",
        "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')"
      ],
      "metadata": {
        "id": "KLB0ZmC3kfT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Cargar una imagen de ejemplo con rostros\n",
        "from urllib.request import urlopen # Para abir URLs\n",
        "\n",
        "# URL de un video con muchos rostros pasando\n",
        "# url = 'https://youtu.be/NKrmicPqy2M?si=cPbsRjNIFlV0FOYq'\n",
        "\n",
        "# Leemos el video\n",
        "\n",
        "# Option 1: Read from a video file\n",
        "# cap = cv2.VideoCapture('your_video.mp4') # Replace 'your_video.mp4' with the path to your video file\n",
        "\n",
        "# Option 2: Read from the live camera (usually index 0)\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Capture a single frame before the loop starts\n",
        "ret, frame = cap.read()\n",
        "\n",
        "if not ret:\n",
        "    print(\"Error: Could not read frame from video source.\")\n",
        "else:\n",
        "    while True:\n",
        "        # Convert the frame to grayscale for face detection\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Detect faces in the grayscale frame\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "        # Draw rectangles around the detected faces\n",
        "        for (x, y, w, h) in faces:\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "            roi_gray = gray[y:y+h, x:x+w]\n",
        "            roi_color = frame[y:y+h, x:x+w]\n",
        "            eyes = eye_cascade.detectMultiScale(roi_gray)\n",
        "            for (ex, ey, ew, eh) in eyes:\n",
        "                cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)\n",
        "\n",
        "        # Display the frame with detected faces\n",
        "        cv2_imshow(frame)\n",
        "\n",
        "        # Break the loop if 'q' is pressed\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "# Release the video capture object and close all windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Display the captured frame with detections after the loop finishes\n",
        "if ret: # Only display if a frame was successfully captured\n",
        "    print(\"Displaying the captured frame with detections:\")\n",
        "    cv2_imshow(frame)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2uzVh8Vkwp5",
        "outputId": "ed7ad94e-273d-49d8-c3d1-8bc30e86950f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Could not read frame from video source.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a87c7993"
      },
      "source": [
        "# Helper function to stream video from the webcam\n",
        "def video_stream():\n",
        "    js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var captureContext;\n",
        "    var displayCanvas;\n",
        "    var displayContext;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function startup() {\n",
        "      displayCanvas = document.createElement('canvas');\n",
        "      displayContext = displayCanvas.getContext('2d');\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureContext = captureCanvas.getContext('2d');\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.autoplay = true;\n",
        "      video.muted = true;\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.appendChild(video);\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      video.srcObject = stream;\n",
        "\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Start capturing and displaying frames\n",
        "      requestAnimationFrame(processFrame);\n",
        "    }\n",
        "\n",
        "    function processFrame() {\n",
        "      if (shutdown) {\n",
        "        stream.getVideoTracks()[0].stop();\n",
        "        video.remove();\n",
        "        div.remove();\n",
        "        return;\n",
        "      }\n",
        "\n",
        "      captureContext.drawImage(video, 0, 0, video.videoWidth, video.videoHeight);\n",
        "      var data = captureCanvas.toDataURL('image/jpeg', 0.8);\n",
        "\n",
        "      displayContext.drawImage(video, 0, 0, displayCanvas.width, displayCanvas.height);\n",
        "      // You can add processing here if needed before displaying\n",
        "\n",
        "      google.colab.kernel.invokeWithCallback({\n",
        "        'functionName': 'handle_frame',\n",
        "        'args': [data],\n",
        "        'autoAck': true\n",
        "      });\n",
        "\n",
        "      requestAnimationFrame(processFrame);\n",
        "    }\n",
        "\n",
        "    function stop() {\n",
        "      shutdown = true;\n",
        "    }\n",
        "\n",
        "    startup();\n",
        "    ''')\n",
        "    display(js)\n",
        "    print('Streaming started. Press stop_video() to stop.')\n",
        "\n",
        "def stop_video():\n",
        "  # Access the google object via the global scope in Javascript\n",
        "  google.colab.output.eval_js('google.colab.output.eval_js(\"stop()\")')\n",
        "  print('Streaming stopped.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ea158ff"
      },
      "source": [
        "# This function will be called by the Javascript code for each frame\n",
        "def handle_frame(data):\n",
        "    # Decode the image using OpenCV\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    nparr = np.frombuffer(binary, np.uint8)\n",
        "    frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "    if frame is None:\n",
        "        print(\"Error: Could not decode frame.\")\n",
        "        return\n",
        "\n",
        "    # Convert the frame to grayscale for face detection\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the grayscale frame\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    # Draw rectangles around the detected faces\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        roi_gray = gray[y:y+h, x:x+w]\n",
        "        roi_color = frame[y:y+h, x:x+w]\n",
        "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
        "        for (ex, ey, ew, eh) in eyes:\n",
        "            cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)\n",
        "\n",
        "    # Display the frame with detected faces\n",
        "    cv2_imshow(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "62997456",
        "outputId": "3c83fc85-d861-451a-d042-b62890f8fb5d"
      },
      "source": [
        "# Start the video stream\n",
        "video_stream()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var captureContext;\n",
              "    var displayCanvas;\n",
              "    var displayContext;\n",
              "\n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "\n",
              "    function startup() {\n",
              "      displayCanvas = document.createElement('canvas');\n",
              "      displayContext = displayCanvas.getContext('2d');\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureContext = captureCanvas.getContext('2d');\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.autoplay = true;\n",
              "      video.muted = true;\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.appendChild(video);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "      video.srcObject = stream;\n",
              "\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Start capturing and displaying frames\n",
              "      requestAnimationFrame(processFrame);\n",
              "    }\n",
              "\n",
              "    function processFrame() {\n",
              "      if (shutdown) {\n",
              "        stream.getVideoTracks()[0].stop();\n",
              "        video.remove();\n",
              "        div.remove();\n",
              "        return;\n",
              "      }\n",
              "\n",
              "      captureContext.drawImage(video, 0, 0, video.videoWidth, video.videoHeight);\n",
              "      var data = captureCanvas.toDataURL('image/jpeg', 0.8);\n",
              "\n",
              "      displayContext.drawImage(video, 0, 0, displayCanvas.width, displayCanvas.height);\n",
              "      // You can add processing here if needed before displaying\n",
              "\n",
              "      google.colab.kernel.invokeWithCallback({\n",
              "        'functionName': 'handle_frame',\n",
              "        'args': [data],\n",
              "        'autoAck': true\n",
              "      });\n",
              "\n",
              "      requestAnimationFrame(processFrame);\n",
              "    }\n",
              "\n",
              "    function stop() {\n",
              "      shutdown = true;\n",
              "    }\n",
              "\n",
              "    startup();\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streaming started. Press stop_video() to stop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "721249c6"
      },
      "source": [
        "Run the cell above to start the real-time video stream with face and eye detection.\n",
        "To stop the stream, run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "1630979f",
        "outputId": "d9e71c8a-89fb-4fac-9852-2d78fe472bb9"
      },
      "source": [
        "# Stop the video stream\n",
        "stop_video()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "TypeError: google.colab.output.eval_js is not a function",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2794034212.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Stop the video stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstop_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3769842946.py\u001b[0m in \u001b[0;36mstop_video\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstop_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0;31m# Access the google object via the global scope in Javascript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m   \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'google.colab.output.eval_js(\"stop()\")'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Streaming stopped.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: google.colab.output.eval_js is not a function"
          ]
        }
      ]
    }
  ]
}